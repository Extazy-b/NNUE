# Конспект по NNUE

## FEN-нотация
0. Нотация, в одной строчке умещающая информацию о расположении фигур на доске и ходящем цвете, а так же дополнительные параметры

1. Расположение фигур имеет следующий синтаксис
```
1. <Piece Placement> ::= <rank8> '/' <rank7> '/' <rank6> '/' <rank5> '/' <rank4> '/' <rank3> '/' <rank2> '/' <rank1>
```
```
2. <ranki> ::= ('8') | ( [<digit17>] <piece> { [<digit17>] <piece> } [<digit17>] )
```
```
3. <piece> ::= <white Piece> | <black Piece>
```
```
4. <digit17> ::= '1' | '2' | '3' | '4' | '5' | '6' | '7'
```
```
5. <white Piece> ::= 'P' | 'N' | 'B' | 'R' | 'Q' | 'K'
```
```
6. <black Piece> ::= 'p' | 'n' | 'b' | 'r' | 'q' | 'k'
```
Таким образом между двумя ```/``` содtржится описание о фигура стоящих на доске или количестве подряд пустых клеток в описываемой строке

## HalfKP
0. На вход сеть получает fen - позицию, из которой вычисляет расположение каждой фигры на доске и то, чей сейчас ход
1. Входными данными сети будут два вектора, каждый из которых описывает ресурсы ходящей и неходящей сейчас стороны
2. Один вектор содержит до 32 чисел
3. Каждое число является индексом отражающим существование на доске комбинации (король даного цвета стоит на данной клетке, данная фигура данного цвета стоит на данной клетке)
4. Всего таких комбинаций для каждой стороны: 64 * 5 * 2 * 64 = 40960
5. В зависимости от того чей сейчас ход сеть передает вектора один на входную матрицу атакующей стороны и один входную матрицу защищающейся стороны
6. Таким образом входной слой - два вектора размера `40960 x 1`

` Эта механика позволяет усилить контекст положения короля, множество раз повторяя его расположение и добавляет контекст различия атаки и защиты за счёт двух разных матриц (взглягуть под разным углом на одно и то же расположение)`

## Первый внутренний слой
0. На вход внутреннего слоя подаются два вектора `40960 x 1` из входного слоя, stm-вектор и sntm-вектор (`side-to-move` и `side-not-to-move`)
1. Каждый из векторов домножается на соответствующую матрицу, каждая размером `256 x 40960`, давая на выходе вектора `256 x 1`
2. Эти вектора подвергаются сдвигу на такие же по размерности вектора (линейное преобразование `y = Ax + B`) 
3. Два полученныз вектора конкатенируются, образуя вектора размерности `512 x 1`
4. Итоговый вектор подвергается `активации` и затем передаётся на следующий слой

**Матицы и вектора состоят из числел типа `int16`, выходной вектор состоит из тех же данных**

## Алгебра умножения матрицы на бинарный разряженный вектор
1. Дано
   - Натуральные числа `n`, `m`, `k`
   - Вектор размерности `nx1`
   - Все компоненты вектора кроме N-штук равны нулю (иные равны `1`)
   - Матрица A размерности `kxn`
2. Строго говоря
``` math
n, m, k \in \N \\
V \in \R^n \\
A \in M(m, n, \R) \\
[A]_{i\,j}  = a_{ij} \\
\exist \; 0 < i_1, ..., i_k \le n: \\
\forall i \neq i_j, v_i = 0 \\
\forall i = i_j, v_i = 1 \\
j \in (1, ..., k)
```
3. Тогда
``` math
[A \cdot V]_l = \sum_{i = 1}^n {a_{l\,i} \cdot u_i} = \sum_{j = 1}^k {a_l\,i_j} \:, l = 1, ..., m
```
4. Таким образом пугающие n x m умножений и m сложений (`n*(m+1) операция`) сводятся до `k*m` сложений

## Квантование
1. После обучения матрицы и вектора весов и сдвигов получаются числами с плавабщей точкой
2. Операции с целыми числами проще и точнее нежели операции с дробными
3. Все коэффициенты каждого слоя умножаются на некое большое Q и затем округляются до целого
4. В системе и в памяти сети будут храниться именно эти целые числа, а так же те самые Q
5. В архитектуре с двумя слоями соответтсвенно будут храниться два чисда QA (внутренний слой) и QB (выхожной слой)
6. QA и QB называются масштабами, т.к. просто масштабируют всю арифметику сети
7. Перед выводом результата его нужно масштабировать обратно, то есть разделить на QA * QB
8. Если во внутреннем слое используется квадратичная RELU активация, то масштаб становится больше, что чревато выходом за границы int16, а именно
```math 
QA^2 \cdot QB
```

## Обновляемость
1. Бинарность входного слоя означает что каждый элемент либо есть либо нет
2. Таким образом "включение" любого нейрона приавляет одно число в каждом элементе вывода акумулятора
3. Отключение в сво очередь вычитает одно число
4. Любой ход (кроме атакующих) опустошает клетку (отключает нейрон соответствующий данной комбинации) и занимает клетку (включает нейрон соответствующий новой комбинации)
5. Таким образом при изменении позиции достаточно
   - сохранить вектор результат акумулятора ```(Y = Ax+b)```
   - узнать индексы неактуально комбинации и появившейся
   - из каждого элемента ```Y``` вычесть элемент из ```A``` находящийся на пересечении соответствующей строки и стобца равного индексу неактуальной комбинации
   - к каждому элементу прибавить элемент найденный аналогичным путём для индекса новой комбинации

```Это даёт выгоду в M раз (вместо К*M операций остаётся K)```

```К атакующим ходам применимы те же правила с поправкой на отключение двух нейронов и включение одного```


## Активация
1. Обычный слой всегда представляет из себя линейное преобразование ``Ax + B``
2. Любое количество композиций линейных преобразований сводится к одному линейному преобразованию 
3. Это является очень простой вичислительной системой, неспосоной к решению сложных и разносторонних задач
4.  Для создания нелинейности применяется функция активации реализуемая различными способами, однако всегда создающая нелинейное преобразование, позволяя создать сложную разветвлённую сеть преобразований компонент
5. Функция активации может иметь одно их следующих воплощений (возможны и другие)
    - ``ReLU(x, a) = max(a, x)`` - клипирование сверху (либо снизу)
    - `` CReLU(x, a, b) = min(max(a, x), b)`` - клипирование с обеих сторон
    - ``Sigmoid`` - размещает вывод на отрезке ``(0, 1)``
    ```math
    \phi(x) = \frac{1}{1+e^{-x}}
    ```
    - ``tanh(x)`` - симметрично размещает вывод на ``[-1, 1]``
   - ``Softmax`` - вероятностное распределение
   ```math
   \phi_{i}(x) = \frac{e^{x_i}}{\sum_j{e^{x_j}}}
   ```
6. Для быстрой работы и быстрого обучения используются ``ReLU`` или ``CReLU``

## Алгебра обучения
### Функция сети (forward pass)
```math
Y = C \times CReLU\big( (A_1 \times X_1 + B_1 \times 1_L) \; || \; (A_2 \times X_2 + B_2 \times 1_L) \big) + d \times 1_L
```

где:
---
```math
X_1, X_2 \in \mathbb{R}^{N \times L} \\ 
входные \: матрицы \: признаков \: (батч \: из \: L \: примеров) \\
```
---
```math
A_1, A_2 \in \mathbb{R}^{K \times N} \\ 
матрицы \: весов \: аккумулятора \: для \: каждой \: стороны \\
```
---
```math
B_1, B_2 \in \mathbb{R}^{K} \\ 
bias \: вектора \: аккумулятора \\
```
---
```math
1_L \in \mathbb{R}^{1 \times L} \\ 
вектор \: единиц \: для \: broadcast \: (повторения \: bias) \\
```
---
```math
|| - конкатенация \: по \: строкам \: (вертикальная) \\
```
---
```math
C \in \mathbb{R}^{1 \times 2K} \\
матрица \: весов \: выходного \: слоя \\
```
---
```math
d \in \mathbb{R} \\ 
сдвиг \: (bias) \: выходного \: слоя \\
```
---
```math
Y \in \mathbb{R}^{1 \times L} \\
модели \: — \: предсказания \: для \: каждого \: из \: L \: примеров \\
```
---
```math
CReLU(x) = \min(\max(x, 0), T) \\ 
clipped \: ReLU \: функция \: активации \\
T \: — \: порог \: (обычно \: 127) 
```
---

### Функция потерь (MSE) по батчу

```math
\text{Loss} = \frac{1}{L} \left\| Y_{\text{ex}} - \left( C \times CReLU\big( (A_1 \times X_1 + B_1 \times 1_L) \; || \; (A_2 \times X_2 + B_2 \times 1_L) \big) + d \times 1_L \right) \right\|^2
```
---
```math
Y_{\text{ex}} \in \mathbb{R}^{1 \times L} \\
матрица \: (вектор) \: ожидаемых \: выходов \: модели \\
```
---
```math
X_1, X_2 \in \mathbb{R}^{N \times L} \\ 
батчи \: входных \: sparse\text{-}векторов \: признаков \\
```
---
```math
A_1, A_2 \in \mathbb{R}^{K \times N} \\ 
веса \: входного \: слоя \: (разные \: для \: каждой \: стороны) \\
```
---
```math
B_1, B_2 \in \mathbb{R}^{K} \\ 
вектора \: bias'ов \: к \: A_1, A_2 \\
```
---
```math
1_L \in \mathbb{R}^{1 \times L} \\
вектор \: единиц \: для \: broadcast'а \: bias \\
```
---
```math
C \in \mathbb{R}^{1 \times 2K} \\
веса \: выходного \: слоя \\
```
---
```math
d \in \mathbb{R} \\
скалярный \: bias \: выходного \: слоя \\
```
---
```math
CReLU(x) = \min(\max(x, 0), T) \\
clipped \: ReLU, \: порог \: T \: (обычно \: 127) \\
```
---
```math
\left\| \cdot \right\|^2 \\
евклидова \: (L_2) \: норма \: по \: всем \: примерам \: батча
```

### Краткие пояснения:

- Входные матрицы \(X_1, X_2\) — каждый столбец — признаки одного примера батча.
- Операция \(A_i \times X_i\) даёт матрицу размером \(K \times L\).
- К bias-вектору \(B_i\) применяется broadcast по столбцам через умножение на \(1_L\).
- Конкатенация создаёт матрицу \(2K \times L\).
- После применения активации и умножения на \(C\) получаем вектор предсказаний \(Y\).
- Ошибка считается как средняя квадратичная по батчу.

### Градиентный спуск
- Алгоритм подбора матиц весов для достижения наименьшей ошибки (в идеале - 0) по всему датасету
```math
1. \text{Задаются начальная точка} \: G_0 =  \{A_1, A_2, B_1, B_2, C, d \} \: \text{и шаг} \lambda\\
2. \text{Вычисляется} Loss(A_1, A_2, B_1, B_2, C, d) \\
3. \text {Если} \: Loss > \epsilon_1 \text{, то} \\
4. \text{Вычисляется } \nabla Loss \:  \text{по правилу} \\
\boxed{
\begin{aligned}
\nabla_{A_1} Loss &= -2 \cdot \operatorname{diag}(C^{(1)}) \cdot \left( ( \Delta \odot M^{(1)} ) \cdot X_1^\top \right) \in \mathbb{R}^{M \times N} \\
\nabla_{B_1} Loss &= -2 \cdot \operatorname{diag}(C^{(1)}) \cdot \left( ( \Delta \odot M^{(1)} ) \cdot \mathbf{1}_L \right) \in \mathbb{R}^{M \times 1} \\
\nabla_{A_2} Loss &= -2 \cdot \operatorname{diag}(C^{(2)}) \cdot \left( ( \Delta \odot M^{(2)} ) \cdot X_2^\top \right) \in \mathbb{R}^{M \times N} \\
\nabla_{B_2} Loss &= -2 \cdot \operatorname{diag}(C^{(2)}) \cdot \left( ( \Delta \odot M^{(2)} ) \cdot \mathbf{1}_L \right) \in \mathbb{R}^{M \times 1} \\
\nabla_C Loss &= -2 \cdot \Delta \cdot H^\top \in \mathbb{R}^{1 \times 2M} \\
\nabla_d Loss &= -2 \cdot \Delta \cdot \mathbf{1}_L \in \mathbb{R}^{1 \times 1} \\
\end{aligned}
} \\ 
5. \text{Вычислить новые коэффициенты по правилу} \\
G_{i + 1} = G_{i} - \lambda \cdot \nabla Loss(G_i) \\
 \{A_1, A_2, B_1, B_2, C, d \} = G_{i+1} \\

6. Если \:\; ||\nabla Loss(G_i)|| < \epsilon_2
```

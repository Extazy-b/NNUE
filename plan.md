## День 1: Модель и DataLoader

### Задача: Реализовать NNUEDataset и make_dataloader в dataset.py.

- __init__: Принять список путей к .npz файлам. Для эффективности можно использовать "ленивую" загрузку (загружать файл только когда до него доходит очередь в __getitem__).

- __len__: Просуммировать количество образцов во всех .npz файлах (можно предварительно сохранить эти цифры в __init__).

- __getitem__: По заданному индексу idx определить, в каком файле он находится, загрузить этот файл и вернуть соответствующий срез данных (X1, X2, Y), преобразованный в torch.tensor.

- make_dataloader: Просканировать директорию npz_dir, создать экземпляр NNUEDataset и обернуть его в torch.utils.data.DataLoader.

### Результат: Возможность итерироваться по батчам данных для обучения.

## День 2: Ядро нейросети

### Задача: Реализовать класс NNUE в model.py.

- Создать класс, наследуемый от torch.nn.Module.

- В __init__: определить параметры модели: A1, A2, B1, B2, C, d. Использовать torch.nn.Parameter.

- Реализовать forward(self, x1, x2): именно тот расчет, который расписан в вашем конспекте.

- Входы: два батча спарс-тензоров x1 и x2 формы (batch_size, INPUT_VECTOR_SIZE).

- Важно: Реализовать эффективное умножение матрицы на спарс-вектор через torch.sparse.mm(). Это будет сильно медленнее, чем будущая C++ реализация, но необходимо для корректной работы.

- Применить crelu (можно использовать torch.clamp).

- Дополнительно: Реализовать функцию инициализации параметров (например, методом Кэйминга).

### Результат: Готовая модель, которую можно протестировать на одном батче.

## День 3-4: Цикл обучения и функция потерь

### Задача: Реализовать train.py.

Написать код для обучения одного эпоха.

Выбрать оптимизатор (например, torch.optim.AdamW или SGD).

Реализовать функцию потерь (MSE).

Добавить логирование (вывод лоса в консоль, можно использовать tqdm для красивого прогресс-бара).

Ключевая задача: Реализовать расчет градиентов и шаг оптимизации согласно вашим формулам. Здесь нужно быть особенно внимательным.

Добавить валидацию: после каждой эпохи прогонять модель на тестовом наборе данных и считать точность/лосс.

Реализовать сохранение чекпоинтов (весов модели) лучшей эпохи.

Результат: Рабочий скрипт, который может обучать модель.

## День 5: Квантование

### Задача: Реализовать quantize.py.

Написать функцию, которая принимает обученную модель (ее веса в float32).

Для каждого параметра найти подходящий масштаб Q (это отдельная задача минимизации ошибки квантования).

Квантовать веса: weight_int16 = torch.round(weight_float * Q).to(torch.int16).

Сохранить квантованные веса и масштабы в файл (например, в формате .json или бинарном). Этот файл — цель всей Python-части, он будет использоваться в C++ движке.

Результат: Инструмент для преобразования обученной модели в формат для эффективного инференса.

## День 6: Интеграция и тестирование

### Задача: Связать все компоненты воедино и протестировать на маленьком датасете.

Написать скрипт, который:

Создает DataLoader из нескольких маленьких .npz файлов.

Инициализирует модель.

Запускает обучение на 1-2 эпохи.

Выводит лосс, убеждаясь, что он уменьшается.

Применяет квантование к полученным весам.

Сохраняет результат.

Написать тесты для ключевых компонентов: прямого прохода модели, операции квантования.

Результат: Полностью рабочий пайплайн, готовый к запуску на полном датасете.

## День 7: Документация и рефакторинг

### Задача: Привести код и документацию в идеальное состояние.

Написать исчерпывающий README.md: как подготовить данные, как запустить обучение.

Обновить Architecture.md, добавив в него схему взаимодействия всех Python-модулей.

Провести рефакторинг кода: удалить дублирование, улучшить читаемость, добавить комментарии.

Упаковать скрипты обучения в функцию с аргументами командной строки (можно использовать argparse).

Результат: Проект, которым можно легко пользоваться и продолжать развитие.
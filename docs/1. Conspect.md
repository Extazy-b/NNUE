# Конспект по NNUE

## FEN-нотация
0. Нотация, в одной строке умещающая информацию о расположении фигур на доске, цвете, который ходит, а также дополнительные параметры.

1. Расположение фигур имеет следующий синтаксис:
```
1. <Piece Placement> ::= <rank8> '/' <rank7> '/' <rank6> '/' <rank5> '/' <rank4> '/' <rank3> '/' <rank2> '/' <rank1>
```
```
2. <ranki> ::= ('8') | ( [<digit17>] <piece> { [<digit17>] <piece> } [<digit17>] )
```
```
3. <piece> ::= <white Piece> | <black Piece>
```
```
4. <digit17> ::= '1' | '2' | '3' | '4' | '5' | '6' | '7'
```
```
5. <white Piece> ::= 'P' | 'N' | 'B' | 'R' | 'Q' | 'K'
```
```
6. <black Piece> ::= 'p' | 'n' | 'b' | 'r' | 'q' | 'k'
```
Таким образом, между двумя `/` содержится описание фигур, стоящих на горизонтали, или количество подряд идущих пустых клеток в описываемой строке.

## HalfKP
0. На вход сеть получает FEN-позицию, из которой вычисляется расположение каждой фигуры на доске и то, чей сейчас ход.
1. Входными данными сети будут два вектора, каждый из которых описывает особенности стороны, которая ходит, и стороны, которая не ходит.
2. Один вектор содержит до 32 чисел (индексов).
3. Каждое число является индексом, отражающим наличие на доске комбинации: «король данного цвета стоит на клетке K», «фигура данного типа и цвета стоит на клетке P».
4. Всего таких комбинаций для каждой стороны: `64 (позиции короля) * 5 (типов фигур: не король) * 2 (цвета фигуры) * 64 (позиции фигуры) = 40960`.
5. В зависимости от того, чей сейчас ход, сеть передаёт один вектор на входную матрицу атакующей стороны (stm - side to move) и один на входную матрицу защищающейся стороны (nstm - not side to move).
6. Таким образом, входной слой — это два разреженных бинарных вектора размерности `40960 x 1`.

`Эта механика позволяет усилить контекст положения короля, многократно повторяя его расположение, и добавляет контекст различия атаки и защиты за счёт двух разных матриц весов (взглянуть под разным углом на одно и то же расположение фигур).`

## Первый внутренний слой (аккумулятор)
0. На вход внутреннего слоя подаются два вектора `40960 x 1` из входного слоя: stm-вектор и nstm-вектор (`side-to-move` и `not-side-to-move`).
1. Каждый из векторов умножается на соответствующую матрицу весов (каждая размером `256 x 40960`), давая на выходе два вектора `256 x 1`.
2. Эти вектора подвергаются сдвигу на вектор bias'а (линейное преобразование `y = Ax + b`).
3. Два полученных вектора конкатенируются, образуя вектор размерности `512 x 1`.
4. Итоговый вектор пропускается через функцию **активации** (например, `CReLU`), и затем результат передаётся на следующий слой.

**Матрицы и вектора состоят из чисел типа `int16`; выходной вектор состоит из данных того же типа.**

## Алгебра умножения матрицы на бинарный разреженный вектор
1. Дано:
   - Натуральные числа `n`, `m`, `k`.
   - Вектор `V` размерности `n x 1`.
   - Все компоненты вектора, кроме `k` штук, равны нулю. Ненулевые компоненты равны `1`.
   - Матрица `A` размерности `m x n`.

2. Формально:
```math
n, m, k \in \N \\
V \in \R^n \\
A \in M(m, n, \R) \\
[A]_{i\,j}  = a_{ij} \\
\exists \; 0 < i_1, ..., i_k \le n: \\
\forall i \notin \{i_1, ..., i_k\},  v_i = 0 \\
\forall i \in \{i_1, ..., i_k\},  v_i = 1 \\
```

3. Тогда результат умножения:
```math
[A \cdot V]_l = \sum_{i = 1}^n {a_{l\,i} \cdot v_i} = \sum_{j = 1}^k {a_{l\,i_j}}, \quad l = 1, ..., m
```
4. Таким образом, пугающие `n * m` умножений и `m * (n-1)` сложений (`~n*m операций`) сводятся всего к `k * m` операциям сложения.

## Квантование
1. После обучения матрицы и вектора весов и сдвигов (biases) являются числами с плавающей точкой.
2. Операции с целыми числами часто выполняются быстрее и точнее (на специализированном железе), чем операции с дробными.
3. Все коэффициенты каждого слоя умножаются на некое большое число `Q` (коэффициент квантования) и затем округляются до целого.
4. В памяти сети хранятся именно эти целые числа, а также значения `Q` для каждого слоя.
5. В архитектуре с двумя слоями, соответственно, будут храниться два числа: `Q_A` (для внутреннего слоя) и `Q_B` (для выходного слоя).
6. `Q_A` и `Q_B` называются масштабами, так как они масштабируют всю арифметику сети.
7. Перед выводом результата его нужно масштабировать обратно, то есть разделить на произведение `Q_A * Q_B`.
8. Если во внутреннем слое используется квадратичная (или иная нелинейная) активация, like в `CReLU`, итоговый масштаб может стать очень большим (`Q_A^2 * Q_B`), что чревато выходом за границы `int16`.

## Обновляемость (Incrementality)
1. Бинарность и разреженность входного слоя означает, что каждый элемент либо присутствует (1), либо отсутствует (0).
2. Таким образом, "включение" любого входного нейрона добавляет соответствующий столбец матрицы весов к результату (аккумулятору).
3. "Отключение" нейрона, в свою очередь, вычитает этот столбец из аккумулятора.
4. Любой ход (кроме взятия) освобождает одну клетку (отключает нейрон, соответствующий старой позиции фигуры) и занимает другую клетку (включает нейрон, соответствующий новой позиции фигуры).
5. Таким образом, при изменении позиции достаточно:
   - Сохранить вектор-результат аккумулятора (`Y = A * X + b`).
   - Определить индексы комбинаций, которые стали неактуальны, и тех, которые появились.
   - Для каждого элемента вектора `Y` вычесть элемент из `A`, находящийся на пересечении строки этого элемента и столбца, равного индексу неактуальной комбинации.
   - К каждому элементу `Y` прибавить элемент, найденный аналогичным путём для индекса новой комбинации.

```Это даёт выгоду в m раз (вместо k*m операций остаётся 2*k операций на ход, где k — количество изменений, обычно 1-2)```.

```К атакующим ходам применимы те же правила с поправкой на отключение двух нейронов (исходная позиция своей фигуры и позиция битой фигуры противника) и включение одного (новая позиция своей фигуры)```.

## Активация
1. Обычный (линейный) слой всегда представляет собой линейное преобразование ``y = Ax + b``.
2. Любая композиция линейных преобразований сама является линейным преобразованием.
3. Это очень простая вычислительная система, неспособная к решению сложных задач.
4. Для создания нелинейности применяется функция активации, реализуемая различными способами. Она создаёт нелинейное преобразование, позволяя сети обучаться сложным нелинейным зависимостям.
5. Функция активации может иметь одно из следующих воплощений (возможны и другие):
    - ``ReLU(x) = \max(0, x)`` — выпрямитель (клипирование снизу).
    - ``CReLU(x, T) = \min(\max(0, x), T)`` — клипированный ReLU (с обеих сторон).
    - ``Sigmoid`` — размещает вывод на интервале ``(0, 1)``.
    ```math
    \sigma(x) = \frac{1}{1+e^{-x}}
    ```
    - ``tanh(x)`` — симметрично размещает вывод на интервале ``[-1, 1]``.
    ```math
    \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
    ```
    - ``Softmax`` — преобразует вектор в распределение вероятностей.
    ```math
    \text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j{e^{x_j}}}
    ```
6. Для быстрой работы и обучения в NNUE чаще всего используются ``ReLU`` или ``CReLU``.

## Алгебра обучения
### Функция сети (прямой проход, forward pass)
```math
Y = C \times \text{CReLU}\left( \left( A_1 \times X_1 + B_1 \times \mathbf{1}_L^\top \right) \Vert \left( A_2 \times X_2 + B_2 \times \mathbf{1}_L^\top \right) \right) + d \times \mathbf{1}_L^\top
```

где:
---
```math
X_1, X_2 \in \mathbb{R}^{N \times L}
```
Входные матрицы признаков (батч из `L` примеров, каждый пример — разреженный вектор размерности `N=40960`).
---
```math
A_1, A_2 \in \mathbb{R}^{K \times N}
```
Матрицы весов аккумулятора для каждой стороны (`K=256`).
---
```math
B_1, B_2 \in \mathbb{R}^{K \times 1}
```
Вектора смещений (bias) аккумулятора.
---
```math
\mathbf{1}_L \in \mathbb{R}^{L \times 1}
```
Вектор-столбец из единиц для broadcast'а (распространения) bias на весь батч.
---
```math
\Vert
```
Обозначает конкатенацию по первому измерению (вертикальную, по строкам). Результат имеет размерность `2K \times L`.
---
```math
C \in \mathbb{R}^{1 \times 2K}
```
Матрица весов выходного слоя.
---
```math
d \in \mathbb{R}
```
Скалярное смещение (bias) выходного слоя.
---
```math
Y \in \mathbb{R}^{1 \times L}
```
Выход модели — предсказания для каждого из `L` примеров батча.
---
```math
\text{CReLU}(x) = \min(\max(x, 0), T)
```
Clipped ReLU функция активации. `T` — порог (обычно `127` для квантованных сетей).
---

### Функция потерь (MSE) по батчу

```math
\text{Loss} = \frac{1}{L} \left\| Y_{\text{target}} - Y_{\text{pred}} \right\|_2^2
```
```math
Y_{\text{target}} \in \mathbb{R}^{1 \times L}
```
Вектор целевых (ожидаемых) значений для батча.
---
```math
Y_{\text{pred}}
```
Предсказание модели, вычисляемое по формуле прямого прохода выше.
---
```math
\left\| \cdot \right\|_2^2
```
Квадрат евклидовой нормы (сумма квадратов разностей по всем `L` примерам батча).
---

### Градиентный спуск
Алгоритм подбора параметров (матриц весов и смещений) для минимизации ошибки на всём датасете.

```math
1. \text{Инициализировать начальные параметры } \Theta_0 = \{A_1, A_2, B_1, B_2, C, d \} \text{ и скорость обучения } \lambda.
```
```math
2. \text{Вычислить } \text{Loss}(\Theta_i).
```
```math
3. \text{Если } \text{Loss} > \epsilon_1 \text{, то:}
```
```math
4. \quad \text{Вычислить градиенты } \nabla \text{Loss}(\Theta_i) \text{ по всем параметрам.}
```
```math
5. \quad \text{Обновить параметры: } \Theta_{i+1} = \Theta_{i} - \lambda \cdot \nabla \text{Loss}(\Theta_i)
```
```math
6. \quad \text{Если } ||\nabla \text{Loss}(\Theta_i)|| < \epsilon_2 \text{, остановиться.}
```
```math
7. \quad \text{Иначе, перейти к шагу 2.}
```

**Формулы для вычисления градиентов (опущен `1/L` множитель из Loss для краткости):**
```math
\boxed{
\begin{aligned}
\Delta &= (Y_{\text{pred}} - Y_{\text{target}}) \in \mathbb{R}^{1 \times L} \\
H &= \text{CReLU}'\left( \left( A_1 X_1 + B_1 \mathbf{1}_L^\top \right) \Vert \left( A_2 X_2 + B_2 \mathbf{1}_L^\top \right) \right) \odot \left( C^\top \Delta \right) \\
M^{(1)} &= H_{1:K, :} \quad \text{(первые K строк матрицы H)} \\
M^{(2)} &= H_{K+1:2K, :} \quad \text{(последние K строк матрицы H)} \\
\\
\nabla_{C} \, \text{Loss} &= \Delta \cdot \left( \text{CReLU}\left( ... \right) \right)^\top \\
\nabla_{d} \, \text{Loss} &= \Delta \cdot \mathbf{1}_L \\
\\
\nabla_{A_1} \, \text{Loss} &= M^{(1)} \cdot X_1^\top \\
\nabla_{B_1} \, \text{Loss} &= M^{(1)} \cdot \mathbf{1}_L \\
\nabla_{A_2} \, \text{Loss} &= M^{(2)} \cdot X_2^\top \\
\nabla_{B_2} \, \text{Loss} &= M^{(2)} \cdot \mathbf{1}_L \\
\end{aligned}
}
```
**Пояснение:** `⊙` обозначает поэлементное умножение (произведение Адамара), а `CReLU'` — это производная функции активации (ноль вне интервала `[0, T]` и единица внутри него).

---
# Gradient calculation

## Conitions
### Dimensions
```math
L, N, M \in \N
```
### Constants
```math
X_1, X_2 \in \R^{N \times L} \\
Y_{exp} \in \R^{1 \times L} \\
t \in \R
1_L = \left(1, \dots, 1 \right) \in \R^{1 \times L}
```
### Vatiables
```math
A_1, A_2 \in \R^{M \times N} \\
B_1, B_2 \in \R^{M \times 1} \\
C \in \R^{1 \times 2M} \\
d \in \R
```
### Ð’esignations
```math
E^i - i_{th} \, element \, of \, column-vector \:E \\
E_j - j_{th} \, element \, of \, row-vector \: E \\
E^i_j - element \, in \, the \, i_{th} \, row \, and \, j_{th} \, column\,  of \, matrix \: E
```
### New operands
```math
\left[ CReLU(A, t) \right]_{ij} = 
\begin{cases}
0, & A_{ij} < 0 \\
A_{ij}, & 0 \le A_{ij} \le t \\
t, & A_{ij} > t
\end{cases} 
```
```math
\left[ A || B \right]_{ij} = 
\begin{cases}
A_{ij}, & i \le m_A \\
B{ij}, & i > m_A
\end{cases}
```
### General expression
```math
Loss(A_1, A_2, B_1, B_2, C, d) = \frac{1}{L} \cdot ||Y_{exp} - (C \cdot CReLU(\left[ A_1 \times X_1 + B_1 \cdot 1_L \right] || \left[ A_2 \times X_2 + B_2 \cdot 1_L \right] , t) + d)||^2_2
```
## Derivatives
### Matrix-by-matrix mult
```math
(\frac{\partial (A \cdot B)}{\partial A})_{i j p q} = \frac{\partial (A \cdot B)_{ij}}{\partial A_{pq}} = \frac{\partial}{\partial A_{pq}}\sum_k A_{ik} \cdot B_{kj} = \delta_{ip} \cdot B_{qj}
```
```math
(\frac{\partial (A \cdot B)}{\partial B})_{i j p q} = \frac{\partial (A \cdot B)_{ij}}{\partial B_{pq}} = \frac{\partial}{\partial B_{pq}}\sum_k A_{ik} \cdot B_{kj} = A_{ip} \cdot \delta_{pj}
```
### Matrix-by-vector mult
```math
(\frac{\partial (A \cdot u)}{\partial A})_{p i j} = \frac{\partial (A \cdot u)_{p}}{\partial A_{ij}} = \frac{\partial}{\partial A_{ij}}\sum_k A_{pk} \cdot u_{k} = \delta_{ip} \cdot u_j
```
```math
(\frac{\partial (A \cdot u)}{\partial u})_{i p} = \frac{\partial (A \cdot u)_{i}}{\partial u_p} = \frac{\partial}{\partial u_p} \sum_k A_{ik} \cdot u_{k} = A_{ip}
```

### Vector-by-matrix mult
```math
(\frac{\partial (u \cdot A)}{\partial A})_{p i j} = \frac{\partial (u \cdot A )_{p}}{\partial A_{ij}} = \frac{\partial}{\partial A_{ij}}\sum_k  u_{k} \cdot A_{kp} = u_i \cdot \delta_{jp}
```
```math
(\frac{\partial (u \cdot A)}{\partial u})_{pj} = \frac{\partial (u \cdot A)_{j}}{\partial u_p} = \frac{\partial}{\partial u_p}\sum_k  u_{k} \cdot A_{kj} = A_{pj}
```

### Matrix concatenation by row
```math
(\frac{\partial (A || B)}{\partial A})_{i j p q} = \frac{\partial (A || B)_{ij}}{\partial A_{ p q}} = \delta_{ip} \cdot \delta_{jq} \ 1_{i \le m_A}
```
```math
(\frac{\partial (A || B)}{\partial B})_{i j p q} = \frac{\partial (A || B)_{ij}}{\partial B_{ p q}} = \delta_{ip} \cdot \delta_{jq} \ 1_{i>m_A}
```

### CReLU from matrix
```math
(\frac{\partial (CReLU(A, t))}{\partial A})_{ijpq} = (\frac{\partial (CReLU(A, t))_{ij}}{\partial A_{pq}}) = \delta_{ip} \cdot \delta_{jq} \cdot 1_{0 < A_{ij} < t} 
```

### Norm
```math
(\frac{\partial \vert\vert A \vert\vert^2_2}{\partial A})_{ij} = 2A_{ij}
```

## Chain rule for multi-dimensions functions    
```math    
Y = F(z_1, \dots, z_n) \\
z_i = G(x_1, \dots, x_k) \\
\frac{\partial Y}{\partial x_p} \\
dY = \sum_{i=1}^n \frac{\partial Y}{\partial z_i} \, dz_i \\
dz_i = \sum_{j=1}^k \frac{\partial z_i}{\partial x_j} \, dx_j \\
dY = \sum_{j=1}^k \, \left[ \sum_{i=1}^n \frac{\partial Y}{\partial z_i} \cdot \frac{\partial z_i}{\partial x_j} \right] dx_j \\ \\
```

## Sub-functions
```math
Loss(\Delta) = \vert\vert\Delta\vert\vert^2_2 \in \R \\
\Delta(Y) = Y_{exp} - Y \in \R^{1 \times L}\\
Y(C, H, d) = C \cdot H + d \cdot 1_L \in \R^{1 \times L} \\
C \in \R^{1 \times 2M} \\
d \in \R \\ 
H(Z) = CReLU(Z, t) \in \R^{2M \times L}\\
t \in \R \\
Z(Z_1, Z_2) = Z_1 \vert \vert Z_2 \in \R^{2M \times L}\\
Z_1(A_1, B_1) = A_1 \cdot X_1 + B_1 \cdot 1_L \in \R^{M \times L}\\
X_1 \in \R^{N \times L} \\
A_1 \in \R^{M \times N} \\
B_1 \in \R^{M \times 1} \\
Z_2(A_2, B_2) = A_2 \cdot X_2 + B_2 \cdot 1_L \in \R^{M \times L} \\
X_2 \in \R^{N \times L} \\
A_2 \in \R^{M \times N} \\
B_2 \in \R^{M \times 1} \\
```
## Partial-deriviation
```math
(\frac{\partial Loss}{\partial \Delta})_{l} = 2\Delta_{l} 
```
---
```math
(\frac{\partial \Delta}{\partial Y })_{ls} = - \delta_{ls}
```
---
```math
(\frac{\partial Y}{\partial C})_{su} = H_{us}
```
```math
(\frac{\partial Y}{\partial H})_{smn} = C_m \cdot \delta_{sn}
```
```math
(\frac{\partial Y}{\partial d})_{s} = 1
```
---
```math
(\frac{\partial H}{\partial Z})_{mnkq} = \delta_{mk} \cdot \delta_{nq} \cdot 1_{0 \le Z_{kq} \le t}
```
---
```math
(\frac{\partial Z}{\partial Z_1})_{kqrc} =  \delta_{kr} \cdot \delta_{qc} \ 1_{k \le M}
```
---
```math
(\frac{\partial Z}{\partial Z_2})_{kqrc} = \delta_{kr} \cdot \delta_{qc} \ 1_{k>M}
```
---
```math
(\frac{\partial Z_1}{\partial A_1})_{rcij} = \delta_{ri} \cdot X_{1 \: jc} 
```
```math
(\frac{\partial Z_1}{\partial B_1})_{rci} = \delta_{ri} 
```
---
```math
(\frac{\partial Z_2}{\partial A_2})_{rcij} = \delta_{ri} \cdot X_{2 \: jc} 
```
```math
(\frac{\partial Z_2}{\partial B_2})_{rci} = \delta_{ri}
```

## Build it all
```math
(\frac{\partial Loss}{\partial A_1})_{ij} = 
\sum_{l=1}^L 2\Delta_{l} 
\sum_{s=1}^L  - \delta_{ls}
\sum_{m=1}^{2M}\sum_{n=1}^L C_m \cdot \delta_{sn}
\sum_{k=1}^{2M}\sum_{q=1}^L \delta_{mk} \cdot \delta_{nq} \cdot 1_{0 \le Z_{kq} \le t}
\sum_{r=1}^M\sum_{c=1}^L \delta_{kr} \cdot \delta_{qc} \cdot 1_{k>M}
\delta_{ri} \cdot X_{1 \: jc} =
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_i \cdot X_{1 \: jc} \cdot 1_{0 \le Z_{ic} \le t}
```
```math
(\frac{\partial Loss}{\partial A_2})_{ij} = 
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_{M+i} \cdot X_{2 \: jc} \cdot 1_{0 \le Z_{ic} \le t} 
```
```math
(\frac{\partial Loss}{\partial B_1})_{i} = 
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_i  \cdot 1_{0 \le Z_{ic} \le t}
```
```math
(\frac{\partial Loss}{\partial B_2})_{i} = 
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_{M+i}  \cdot 1_{0 \le Z_{ic} \le t}
```
```math
(\frac{\partial Loss}{\partial C})_{u} = 
-2 \cdot \sum_{s=1}^L
\Delta_{s} \cdot H_{us}
```
```math
(\frac{\partial Loss}{\partial d})_{u} = 
-2 \cdot \sum_{s=1}^L
\Delta_{s}
```

## Matrix veiw
```math
\Delta \in \R^{1 \times L} - \text{error vector} \\
C \in \R^{1 \times 2M} - \text{second layer weights vecotr} \\
X_1, X_2 \in \R^{N \times L} - \text{input vectors} \\
M \in \{0, 1\}^{2M \times L} - \text{mask by Z-elements}\\
M_{ij} = \begin{cases} 1, & 0 \le Z_{ij} \le t \\ 0,& Z_{ij} < 0 \:\vert\: Z_{ij} > t \end{cases} \\
H = CReLU(Z, t) \in \R^{2M \ times L} \\
1_L \in  \R^{L \times 1} - \text{row vector of ones}
```
---
```math
C^{(1)} = C_{[:, 0:M]} \in \R^{1 \times M}, \:\:\:\:\: C^{(2)} = C_{[:, M:M2]} \in \R^{1 \times M} \\
M^{(1)} = M_{[0:M, :]} \in \R^{M \times ML}, \:\:\:\:\: M^{(2)} = M_{[M:M2, :]} \in \R^{M \times L}
```
---
```math
\odot - \text{element-by-element product} \\
diag(\cdot) - \text{diagonal matrix from vector}
```
---
```math
\boxed{
\begin{aligned}
\nabla_{A_1} Loss &= -2 \cdot \operatorname{diag}(C^{(1)}) \cdot \left( ( \Delta \odot M^{(1)} ) \cdot X_1^\top \right) \in \mathbb{R}^{M \times N} \\
\nabla_{B_1} Loss &= -2 \cdot \operatorname{diag}(C^{(1)}) \cdot \left( ( \Delta \odot M^{(1)} ) \cdot \mathbf{1}_L \right) \in \mathbb{R}^{M \times 1} \\
\nabla_{A_2} Loss &= -2 \cdot \operatorname{diag}(C^{(2)}) \cdot \left( ( \Delta \odot M^{(2)} ) \cdot X_2^\top \right) \in \mathbb{R}^{M \times N} \\
\nabla_{B_2} Loss &= -2 \cdot \operatorname{diag}(C^{(2)}) \cdot \left( ( \Delta \odot M^{(2)} ) \cdot \mathbf{1}_L \right) \in \mathbb{R}^{M \times 1} \\
\nabla_C Loss &= -2 \cdot \Delta \cdot H^\top \in \mathbb{R}^{1 \times 2M} \\
\nabla_d Loss &= -2 \cdot \Delta \cdot \mathbf{1}_L \in \mathbb{R}^{1 \times 1} \\
\end{aligned}
}
```
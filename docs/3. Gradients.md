# Gradient calculation

## Conitions
### Dimensions
```math
L, N, M \in \N
```
### Constants
```math
X_1, X_2 \in \R^{N \times L} \\
Y_{exp} \in \R^{1 \times L} \\
t \in \R
1_L = \left(1, \dots, 1 \right) \in \R^{1 \times L}
```
### Vatiables
```math
A_1, A_2 \in \R^{M \times N} \\
B_1, B_2 \in \R^{M \times 1} \\
C \in \R^{1 \times 2M} \\
d \in \R
```
### Ð’esignations
```math
E^i - i_{th} \, element \, of \, column-vector \:E \\
E_j - j_{th} \, element \, of \, row-vector \: E \\
E^i_j - element \, in \, the \, i_{th} \, row \, and \, j_{th} \, column\,  of \, matrix \: E
```
### New operands
```math
\left[ CReLU(A, t) \right]_{ij} = 
\begin{cases}
0, & A_{ij} < 0 \\
A_{ij}, & 0 \le A_{ij} \le t \\
t, & A_{ij} > t
\end{cases} 
```
```math
\left[ A || B \right]_{ij} = 
\begin{cases}
A_{ij}, & i \le m_A \\
B{ij}, & i > m_A
\end{cases}
```
### General expression
```math
Loss(A_1, A_2, B_1, B_2, C, d) = \frac{1}{L} \cdot ||Y_{exp} - (C \cdot CReLU(\left[ A_1 \times X_1 + B_1 \cdot 1_L \right] || \left[ A_2 \times X_2 + B_2 \cdot 1_L \right] , t) + d)||^2_2
```
## Derivatives
### Matrix-by-matrix mult
```math
(\frac{\partial (A \cdot B)}{\partial A})_{i j p q} = \frac{\partial (A \cdot B)_{ij}}{\partial A_{pq}} = \frac{\partial}{\partial A_{pq}}\sum_k A_{ik} \cdot B_{kj} = \delta_{ip} \cdot B_{qj}
```
```math
(\frac{\partial (A \cdot B)}{\partial B})_{i j p q} = \frac{\partial (A \cdot B)_{ij}}{\partial B_{pq}} = \frac{\partial}{\partial B_{pq}}\sum_k A_{ik} \cdot B_{kj} = A_{ip} \cdot \delta_{pj}
```
### Matrix-by-vector mult
```math
(\frac{\partial (A \cdot u)}{\partial A})_{p i j} = \frac{\partial (A \cdot u)_{p}}{\partial A_{ij}} = \frac{\partial}{\partial A_{ij}}\sum_k A_{pk} \cdot u_{k} = \delta_{ip} \cdot u_j
```
```math
(\frac{\partial (A \cdot u)}{\partial u})_{i p} = \frac{\partial (A \cdot u)_{i}}{\partial u_p} = \frac{\partial}{\partial u_p} \sum_k A_{ik} \cdot u_{k} = A_{ip}
```

### Vector-by-matrix mult
```math
(\frac{\partial (u \cdot A)}{\partial A})_{p i j} = \frac{\partial (u \cdot A )_{p}}{\partial A_{ij}} = \frac{\partial}{\partial A_{ij}}\sum_k  u_{k} \cdot A_{kp} = u_i \cdot \delta_{jp}
```
```math
(\frac{\partial (u \cdot A)}{\partial u})_{pj} = \frac{\partial (u \cdot A)_{j}}{\partial u_p} = \frac{\partial}{\partial u_p}\sum_k  u_{k} \cdot A_{kj} = A_{pj}
```

### Matrix concatenation by row
```math
(\frac{\partial (A || B)}{\partial A})_{i j p q} = \frac{\partial (A || B)_{ij}}{\partial A_{ p q}} = \delta_{ip} \cdot \delta_{jq} \ 1_{i \le m_A}
```
```math
(\frac{\partial (A || B)}{\partial B})_{i j p q} = \frac{\partial (A || B)_{ij}}{\partial B_{ p q}} = \delta_{ip} \cdot \delta_{jq} \ 1_{i>m_A}
```

### CReLU from matrix
```math
(\frac{\partial (CReLU(A, t))}{\partial A})_{ijpq} = (\frac{\partial (CReLU(A, t))_{ij}}{\partial A_{pq}}) = \delta_{ip} \cdot \delta_{jq} \cdot 1_{0 < A_{ij} < t} 
```

### Norm
```math
(\frac{\partial \vert\vert A \vert\vert^2_2}{\partial A})_{ij} = 2A_{ij}
```

## Chain rule for multi-dimensions functions    
```math    
Y = F(z_1, \dots, z_n) \\
z_i = G(x_1, \dots, x_k) \\
\frac{\partial Y}{\partial x_p} \\
dY = \sum_{i=1}^n \frac{\partial Y}{\partial z_i} \, dz_i \\
dz_i = \sum_{j=1}^k \frac{\partial z_i}{\partial x_j} \, dx_j \\
dY = \sum_{j=1}^k \, \left[ \sum_{i=1}^n \frac{\partial Y}{\partial z_i} \cdot \frac{\partial z_i}{\partial x_j} \right] dx_j \\ \\
```

## Sub-functions
```math
Loss(\Delta) = \vert\vert\Delta\vert\vert^2_2 \in \R \\
\Delta(Y) = Y_{exp} - Y \in \R^{1 \times L}\\
Y(C, H, d) = C \cdot H + d \cdot 1_L \in \R^{1 \times L} \\
C \in \R^{1 \times 2M} \\
d \in \R \\ 
H(Z) = CReLU(Z, t) \in \R^{2M \times L}\\
t \in \R \\
Z(Z_1, Z_2) = Z_1 \vert \vert Z_2 \in \R^{2M \times L}\\
Z_1(A_1, B_1) = A_1 \cdot X_1 + B_1 \cdot 1_L \in \R^{M \times L}\\
X_1 \in \R^{N \times L} \\
A_1 \in \R^{M \times N} \\
B_1 \in \R^{M \times 1} \\
Z_2(A_2, B_2) = A_2 \cdot X_2 + B_2 \cdot 1_L \in \R^{M \times L} \\
X_2 \in \R^{N \times L} \\
A_2 \in \R^{M \times N} \\
B_2 \in \R^{M \times 1} \\
```
## Partial-deriviation
```math
(\frac{\partial Loss}{\partial \Delta})_{l} = 2\Delta_{l} 
```
---
```math
(\frac{\partial \Delta}{\partial Y })_{ls} = - \delta_{ls}
```
---
```math
(\frac{\partial Y}{\partial C})_{su} = H_{us}
```
```math
(\frac{\partial Y}{\partial H})_{smn} = C_m \cdot \delta_{sn}
```
```math
(\frac{\partial Y}{\partial d})_{s} = 1
```
---
```math
(\frac{\partial H}{\partial Z})_{mnkq} = \delta_{mk} \cdot \delta_{nq} \cdot 1_{0 \le Z_{kq} \le t}
```
---
```math
(\frac{\partial Z}{\partial Z_1})_{kqrc} =  \delta_{kr} \cdot \delta_{qc} \ 1_{k \le M}
```
---
```math
(\frac{\partial Z}{\partial Z_2})_{kqrc} = \delta_{kr} \cdot \delta_{qc} \ 1_{k>M}
```
---
```math
(\frac{\partial Z_1}{\partial A_1})_{rcij} = \delta_{ri} \cdot X_{1 \: jc} 
```
```math
(\frac{\partial Z_1}{\partial B_1})_{rci} = \delta_{ri} 
```
---
```math
(\frac{\partial Z_2}{\partial A_2})_{rcij} = \delta_{ri} \cdot X_{2 \: jc} 
```
```math
(\frac{\partial Z_2}{\partial B_2})_{rci} = \delta_{ri}
```

## Build it all
```math
(\frac{\partial Loss}{\partial A_1})_{ij} = 
\sum_{l=1}^L 2\Delta_{l} 
\sum_{s=1}^L  - \delta_{ls}
\sum_{m=1}^{2M}\sum_{n=1}^L C_m \cdot \delta_{sn}
\sum_{k=1}^{2M}\sum_{q=1}^L \delta_{mk} \cdot \delta_{nq} \cdot 1_{0 \le Z_{kq} \le t}
\sum_{r=1}^M\sum_{c=1}^L \delta_{kr} \cdot \delta_{qc} \cdot 1_{k>M}
\delta_{ri} \cdot X_{1 \: jc} =
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_i \cdot X_{1 \: jc} \cdot 1_{0 \le Z_{ic} \le t}
```
```math
(\frac{\partial Loss}{\partial A_2})_{ij} = 
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_{M+i} \cdot X_{2 \: jc} \cdot 1_{0 \le Z_{ic} \le t} 
```
```math
(\frac{\partial Loss}{\partial B_1})_{i} = 
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_i  \cdot 1_{0 \le Z_{ic} \le t}
```
```math
(\frac{\partial Loss}{\partial B_2})_{i} = 
-2 \cdot \sum_{c=1}^L 
\Delta_{c} \cdot  C_{M+i}  \cdot 1_{0 \le Z_{ic} \le t}
```
```math
(\frac{\partial Loss}{\partial C})_{u} = 
-2 \cdot \sum_{s=1}^L
\Delta_{s} \cdot H_{us}
```
```math
(\frac{\partial Loss}{\partial d})_{u} = 
-2 \cdot \sum_{s=1}^L
\Delta_{s}
```
## Compactisation

### By-index view

```math
\begin{aligned}
(\frac{\partial Loss}{\partial A_1})_{ij} &= -2 \sum_{c=1}^L \Delta_c \cdot C_i \cdot X_{1 \: j c} \cdot 1_{0 \le Z_{i c} \le t} \\
(\frac{\partial Loss}{\partial A_2})_{ij} &= -2 \sum_{c=1}^L \Delta_c \cdot C_{M+i} \cdot X_{2 \: j c} \cdot 1_{0 \le Z_{i c} \le t} \\
(\frac{\partial Loss}{\partial B_1})_{i} &= -2 \sum_{c=1}^L \Delta_c \cdot C_i \cdot 1_{0 \le Z_{i c} \le t} \\
(\frac{\partial Loss}{\partial B_2})_{i} &= -2 \sum_{c=1}^L \Delta_c \cdot C_{M+i} \cdot 1_{0 \le Z_{i c} \le t} \\
(\frac{\partial Loss}{\partial C})_u &= -2 \sum_{s=1}^L \Delta_s \cdot H_{u s} \\
(\frac{\partial Loss}{\partial d})_u &= -2 \sum_{s=1}^L \Delta_s
\end{aligned}
```

```math
\tilde{X}_1 = 
\begin{bmatrix} X_1 \\ 1_L^\top \end{bmatrix} \in \mathbb{R}^{(N+1) \times L}, 
\quad
\tilde{X}_2 = 
\begin{bmatrix} X_2 \\ 1_L^\top \end{bmatrix} \in \mathbb{R}^{(N+1) \times L}
```

```math
\frac{\partial Loss}{\partial [A_1 | B_1]} = -2 \, \operatorname{diag}(C^{(1)}) \, (\Delta \odot M^{(1)}) \tilde{X}_1^\top
```

```math
\frac{\partial Loss}{\partial [A_2 | B_2]} = -2 \, \operatorname{diag}(C^{(2)}) \, (\Delta \odot M^{(2)}) \tilde{X}_2^\top
```
```math
\nabla_{W_1} =
\begin{bmatrix}
\frac{\partial Loss}{\partial [A_1 | B_1]} \\
\frac{\partial Loss}{\partial [A_2 | B_2]}
\end{bmatrix}
=
-2
\begin{bmatrix}
\operatorname{diag}(C^{(1)}) (\Delta \odot M^{(1)}) \tilde{X}_1^\top \\
\operatorname{diag}(C^{(2)}) (\Delta \odot M^{(2)}) \tilde{X}_2^\top
\end{bmatrix}
\in \mathbb{R}^{2M \times (N+1)}
```
```math
\nabla_{W_2} = \begin{bmatrix} \frac{\partial Loss}{\partial C} & \frac{\partial Loss}{\partial d} \end{bmatrix} 
= -2 \begin{bmatrix} \Delta \cdot H^\top & \Delta \cdot 1_L \end{bmatrix} \in \mathbb{R}^{1 \times (2M+1)}
```